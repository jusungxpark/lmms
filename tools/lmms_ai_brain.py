#!/usr/bin/env python3
"""
LMMS AI Brain - Intelligent music production system using GPT models
This system interprets musical intent and makes intelligent production decisions
"""

import os
import sys
import json
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import random
from pathlib import Path

try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("OpenAI library not available, using rule-based system")

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

# Import our controllers
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from lmms_actions import LMMSActions, Note, Pattern, EditMode, QuantizeMode
from lmms_complete_controller import TrackType, WaveForm, FilterType


@dataclass
class MusicalIntent:
    """Represents the interpreted musical intent from user request"""
    genre: Optional[str] = None
    mood: Optional[str] = None
    energy_level: float = 0.5  # 0-1
    complexity: float = 0.5  # 0-1
    tempo: Optional[int] = None
    key: Optional[str] = None
    time_signature: Optional[Tuple[int, int]] = None
    elements: List[str] = None  # drums, bass, lead, etc.
    effects_intensity: float = 0.5  # 0-1
    characteristics: List[str] = None  # heavy, distorted, minimal, etc.
    reference_tracks: List[str] = None
    duration_bars: int = 8
    specific_requirements: Dict[str, Any] = None


@dataclass
class ProductionPlan:
    """A detailed production plan generated by AI"""
    tracks: List[Dict[str, Any]]
    patterns: List[Dict[str, Any]]
    effects: List[Dict[str, Any]]
    automation: List[Dict[str, Any]]
    mix_settings: Dict[str, Any]
    arrangement: Dict[str, Any]


class MusicKnowledgeBase:
    """Musical knowledge for parameter mapping"""
    
    GENRE_CHARACTERISTICS = {
        "techno": {
            "tempo_range": (120, 140),
            "common_elements": ["kick", "bass", "hats", "clap"],
            "kick_style": "punchy",
            "bass_style": "rolling",
            "effects": ["distortion", "delay", "reverb"],
            "energy": 0.8,
            "complexity": 0.6
        },
        "house": {
            "tempo_range": (120, 130),
            "common_elements": ["kick", "bass", "hats", "clap", "chords"],
            "kick_style": "deep",
            "bass_style": "groovy",
            "effects": ["compression", "eq", "reverb"],
            "energy": 0.7,
            "complexity": 0.5
        },
        "dnb": {
            "tempo_range": (160, 180),
            "common_elements": ["kick", "snare", "bass", "reese"],
            "kick_style": "punchy",
            "bass_style": "sub",
            "effects": ["distortion", "eq", "compression"],
            "energy": 0.9,
            "complexity": 0.8
        },
        "ambient": {
            "tempo_range": (60, 100),
            "common_elements": ["pad", "texture", "lead"],
            "effects": ["reverb", "delay", "chorus"],
            "energy": 0.2,
            "complexity": 0.4
        },
        "trap": {
            "tempo_range": (130, 150),
            "common_elements": ["808", "hihat", "snare"],
            "kick_style": "808",
            "effects": ["distortion", "compression"],
            "energy": 0.7,
            "complexity": 0.6
        }
    }
    
    MOOD_MAPPINGS = {
        "aggressive": {
            "distortion": 0.8,
            "compression_ratio": 8,
            "filter_resonance": 0.8,
            "velocity_range": (80, 127)
        },
        "dark": {
            "filter_cutoff": 0.3,
            "reverb_size": 0.7,
            "pitch_offset": -12,
            "velocity_range": (40, 90)
        },
        "uplifting": {
            "filter_cutoff": 0.8,
            "reverb_size": 0.4,
            "pitch_offset": 0,
            "velocity_range": (60, 110)
        },
        "minimal": {
            "element_density": 0.3,
            "effect_amount": 0.2,
            "velocity_range": (50, 80)
        },
        "chaotic": {
            "randomization": 0.8,
            "effect_amount": 0.9,
            "velocity_range": (20, 127)
        }
    }
    
    EFFECT_INTENSITY_MAPPINGS = {
        "heavy": {
            "distortion": {"dist": 0.9, "gain": 2.0},
            "bitcrush": {"bits": 6, "rate": 8000},
            "compression": {"ratio": 10, "threshold": -20}
        },
        "moderate": {
            "distortion": {"dist": 0.5, "gain": 1.3},
            "bitcrush": {"bits": 12, "rate": 22050},
            "compression": {"ratio": 4, "threshold": -12}
        },
        "light": {
            "distortion": {"dist": 0.2, "gain": 1.1},
            "bitcrush": {"bits": 16, "rate": 44100},
            "compression": {"ratio": 2, "threshold": -6}
        }
    }
    
    @staticmethod
    def get_instrument_for_element(element: str, characteristics: List[str]) -> Dict[str, Any]:
        """Choose instrument and settings based on element type and characteristics"""
        
        element_lower = element.lower()
        
        if "kick" in element_lower or "bd" in element_lower:
            if "heavy" in characteristics or "hard" in characteristics:
                return {
                    "instrument": "kicker",
                    "params": {
                        "start": 200, "end": 40, "decay": 300,
                        "dist": 0.7, "startclick": 0.6, "gain": 1.3
                    }
                }
            elif "deep" in characteristics or "sub" in characteristics:
                return {
                    "instrument": "kicker",
                    "params": {
                        "start": 100, "end": 30, "decay": 500,
                        "dist": 0.2, "startclick": 0.1, "gain": 1.0
                    }
                }
            else:
                return {
                    "instrument": "kicker",
                    "params": {
                        "start": 150, "end": 35, "decay": 350,
                        "dist": 0.4, "startclick": 0.3, "gain": 1.0
                    }
                }
        
        elif "bass" in element_lower:
            if "acid" in characteristics or "303" in element_lower:
                return {
                    "instrument": "lb302",
                    "params": {
                        "cutoff": 80, "reso": 100, "envmod": 80,
                        "decay": 60, "dist": 40
                    }
                }
            elif "sub" in characteristics or "deep" in characteristics:
                return {
                    "instrument": "tripleoscillator",
                    "params": {
                        "vol0": 100, "wavetype0": WaveForm.SINE,
                        "coarse0": -24, "vol1": 30, "wavetype1": WaveForm.SQUARE,
                        "coarse1": -24
                    }
                }
            else:
                return {
                    "instrument": "tripleoscillator",
                    "params": {
                        "vol0": 100, "wavetype0": WaveForm.SAW,
                        "coarse0": -12, "vol1": 50, "wavetype1": WaveForm.SQUARE,
                        "coarse1": -12
                    }
                }
        
        elif "hat" in element_lower or "hh" in element_lower:
            return {
                "instrument": "kicker",
                "params": {
                    "start": 10000, "end": 5000, "decay": 50,
                    "dist": 0.1, "noise": 0.8, "gain": 0.7
                }
            }
        
        elif "snare" in element_lower or "sd" in element_lower:
            return {
                "instrument": "kicker",
                "params": {
                    "start": 400, "end": 200, "decay": 150,
                    "dist": 0.3, "noise": 0.5, "gain": 0.9
                }
            }
        
        elif "pad" in element_lower or "atmosphere" in element_lower:
            return {
                "instrument": "tripleoscillator",
                "params": {
                    "vol0": 60, "wavetype0": WaveForm.SAW,
                    "vol1": 50, "wavetype1": WaveForm.SAW,
                    "finel1": 5, "finer1": -5,
                    "vol2": 40, "wavetype2": WaveForm.SINE,
                    "coarse2": 12
                }
            }
        
        elif "lead" in element_lower or "melody" in element_lower:
            return {
                "instrument": "tripleoscillator",
                "params": {
                    "vol0": 80, "wavetype0": WaveForm.SAW,
                    "vol1": 60, "wavetype1": WaveForm.SQUARE,
                    "finel1": 7
                }
            }
        
        else:
            # Default synth
            return {
                "instrument": "tripleoscillator",
                "params": {
                    "vol0": 70, "wavetype0": WaveForm.SAW,
                    "vol1": 50, "wavetype1": WaveForm.SQUARE
                }
            }


class LMMSAIBrain:
    """
    The intelligent brain that interprets musical requests and generates production plans
    Uses GPT models to understand intent and make musical decisions
    """
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if self.api_key and HAS_OPENAI:
            openai.api_key = self.api_key
        elif self.api_key and not HAS_OPENAI:
            print("Warning: API key provided but OpenAI library not available")
            self.api_key = None
        
        self.knowledge = MusicKnowledgeBase()
        self.controller = LMMSActions()
    
    def interpret_request(self, request: str) -> MusicalIntent:
        """
        Use GPT to interpret the musical intent from natural language
        """
        if not self.api_key:
            # Fallback to rule-based interpretation
            return self._rule_based_interpretation(request)
        
        prompt = f"""
        Analyze this music production request and extract the musical intent.
        Request: "{request}"
        
        Return a JSON object with:
        - genre: detected genre (techno, house, dnb, trap, ambient, etc.)
        - mood: overall mood (aggressive, dark, uplifting, minimal, chaotic, etc.)
        - energy_level: 0.0 to 1.0
        - complexity: 0.0 to 1.0
        - tempo: BPM if specified or implied by genre
        - key: musical key if specified
        - time_signature: [numerator, denominator] if specified
        - elements: list of musical elements needed (kick, bass, hats, lead, pad, etc.)
        - effects_intensity: 0.0 to 1.0 based on descriptions like "heavy", "distorted"
        - characteristics: list of descriptive words (heavy, distorted, minimal, rolling, etc.)
        - duration_bars: number of bars requested
        - specific_requirements: any specific technical requirements
        
        Be intelligent about inferring from context. For example:
        - "heavy" implies high effects_intensity and energy_level
        - "minimal" implies low complexity
        - "fast" implies higher tempo
        - "distorted" should be in characteristics and implies high effects_intensity
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a music production expert who understands genres, moods, and technical requirements."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )
            
            intent_data = json.loads(response.choices[0].message.content)
            
            return MusicalIntent(
                genre=intent_data.get("genre"),
                mood=intent_data.get("mood"),
                energy_level=intent_data.get("energy_level", 0.5),
                complexity=intent_data.get("complexity", 0.5),
                tempo=intent_data.get("tempo"),
                key=intent_data.get("key"),
                time_signature=tuple(intent_data["time_signature"]) if intent_data.get("time_signature") else None,
                elements=intent_data.get("elements", []),
                effects_intensity=intent_data.get("effects_intensity", 0.5),
                characteristics=intent_data.get("characteristics", []),
                duration_bars=intent_data.get("duration_bars", 8),
                specific_requirements=intent_data.get("specific_requirements", {})
            )
            
        except Exception as e:
            print(f"GPT interpretation failed: {e}, falling back to rule-based")
            return self._rule_based_interpretation(request)
    
    def _rule_based_interpretation(self, request: str) -> MusicalIntent:
        """Fallback rule-based interpretation when GPT is not available"""
        request_lower = request.lower()
        
        intent = MusicalIntent()
        
        # Detect genre
        for genre in ["techno", "house", "dnb", "trap", "ambient"]:
            if genre in request_lower:
                intent.genre = genre
                break
        
        # Detect mood and characteristics
        intent.characteristics = []
        if "heavy" in request_lower:
            intent.characteristics.append("heavy")
            intent.effects_intensity = 0.8
            intent.energy_level = 0.8
        if "distorted" in request_lower or "distortion" in request_lower:
            intent.characteristics.append("distorted")
            intent.effects_intensity = max(intent.effects_intensity, 0.7)
        if "minimal" in request_lower:
            intent.characteristics.append("minimal")
            intent.complexity = 0.3
        if "fast" in request_lower:
            intent.energy_level = 0.8
            if not intent.tempo:
                intent.tempo = 140
        if "slow" in request_lower:
            intent.energy_level = 0.3
            if not intent.tempo:
                intent.tempo = 90
        if "dark" in request_lower:
            intent.mood = "dark"
        if "aggressive" in request_lower:
            intent.mood = "aggressive"
            intent.energy_level = 0.9
        
        # Detect elements
        intent.elements = []
        element_keywords = {
            "kick": ["kick", "bd", "bassdrum"],
            "bass": ["bass", "sub", "low"],
            "hats": ["hat", "hh", "hihat"],
            "snare": ["snare", "sd", "clap"],
            "lead": ["lead", "melody", "synth"],
            "pad": ["pad", "atmosphere", "ambient"]
        }
        
        for element, keywords in element_keywords.items():
            if any(kw in request_lower for kw in keywords):
                intent.elements.append(element)
        
        # If no elements specified, use genre defaults
        if not intent.elements and intent.genre:
            genre_data = self.knowledge.GENRE_CHARACTERISTICS.get(intent.genre, {})
            intent.elements = genre_data.get("common_elements", ["kick", "bass", "hats"])
        elif not intent.elements:
            intent.elements = ["kick", "bass", "hats"]  # Default
        
        # Detect duration
        if "loop" in request_lower:
            intent.duration_bars = 4
        elif "8 bar" in request_lower or "eight bar" in request_lower:
            intent.duration_bars = 8
        elif "16 bar" in request_lower:
            intent.duration_bars = 16
        
        return intent
    
    def generate_production_plan(self, intent: MusicalIntent) -> ProductionPlan:
        """
        Generate a detailed production plan based on musical intent
        This can use GPT for more intelligent planning or fall back to rule-based
        """
        if self.api_key:
            return self._gpt_production_plan(intent)
        else:
            return self._rule_based_production_plan(intent)
    
    def _gpt_production_plan(self, intent: MusicalIntent) -> ProductionPlan:
        """Use GPT to generate an intelligent production plan"""
        
        prompt = f"""
        Create a detailed music production plan based on this intent:
        {json.dumps(asdict(intent), indent=2)}
        
        Generate a JSON production plan with:
        
        1. tracks: Array of track definitions, each with:
           - name: track name
           - element: type (kick, bass, hats, etc.)
           - instrument: LMMS instrument to use
           - parameters: instrument-specific parameters
           - volume: 0-100
           - panning: -100 to 100
           - effects: array of effects with parameters
        
        2. patterns: Array of pattern definitions:
           - track: track name
           - name: pattern name
           - notes: array of note events with pitch, position, length, velocity
           - style: pattern style description
        
        3. effects: Global effects and processing
        
        4. mix_settings: Mixing parameters
        
        5. arrangement: Song structure
        
        Be creative and musically intelligent. Consider:
        - Genre conventions
        - Energy and complexity levels
        - Effect intensity
        - Musical coherence
        - Rhythm patterns appropriate for the style
        
        For heavy/distorted requests, use aggressive parameters.
        For minimal requests, use sparse patterns.
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert music producer who knows LMMS intimately. Generate detailed, musically coherent production plans."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000
            )
            
            plan_data = json.loads(response.choices[0].message.content)
            
            return ProductionPlan(
                tracks=plan_data.get("tracks", []),
                patterns=plan_data.get("patterns", []),
                effects=plan_data.get("effects", []),
                automation=plan_data.get("automation", []),
                mix_settings=plan_data.get("mix_settings", {}),
                arrangement=plan_data.get("arrangement", {})
            )
            
        except Exception as e:
            print(f"GPT production planning failed: {e}, using rule-based")
            return self._rule_based_production_plan(intent)
    
    def _rule_based_production_plan(self, intent: MusicalIntent) -> ProductionPlan:
        """Generate production plan using rules and knowledge base"""
        
        tracks = []
        patterns = []
        effects = []
        
        # Determine tempo
        tempo = intent.tempo
        if not tempo and intent.genre:
            genre_data = self.knowledge.GENRE_CHARACTERISTICS.get(intent.genre, {})
            tempo_range = genre_data.get("tempo_range", (120, 130))
            tempo = int((tempo_range[0] + tempo_range[1]) / 2)
        else:
            tempo = 128  # Default
        
        # Create tracks for each element
        for element in intent.elements:
            
            # Get instrument configuration
            inst_config = self.knowledge.get_instrument_for_element(element, intent.characteristics)
            
            # Determine track parameters based on intent
            volume = 100
            if "minimal" in intent.characteristics:
                volume = 60 if element != "kick" else 80
            elif "heavy" in intent.characteristics:
                volume = 100 if element in ["kick", "bass"] else 85
            
            track_effects = []
            
            # Add effects based on intensity and characteristics
            if intent.effects_intensity > 0.6:
                if element in ["kick", "bass"]:
                    if "distorted" in intent.characteristics or intent.effects_intensity > 0.7:
                        effect_level = "heavy" if intent.effects_intensity > 0.8 else "moderate"
                        distortion_params = self.knowledge.EFFECT_INTENSITY_MAPPINGS[effect_level]["distortion"]
                        track_effects.append({
                            "type": "distortion",
                            "params": distortion_params
                        })
                    
                    if element == "bass":
                        track_effects.append({
                            "type": "bassbooster",
                            "params": {"freq": 80, "gain": 10, "ratio": 2}
                        })
                
                if intent.effects_intensity > 0.7:
                    effect_level = "heavy" if intent.effects_intensity > 0.8 else "moderate"
                    bitcrush_params = self.knowledge.EFFECT_INTENSITY_MAPPINGS[effect_level]["bitcrush"]
                    track_effects.append({
                        "type": "bitcrush",
                        "params": bitcrush_params
                    })
            
            tracks.append({
                "name": element.capitalize(),
                "element": element,
                "instrument": inst_config["instrument"],
                "parameters": inst_config["params"],
                "volume": volume,
                "panning": 0,
                "effects": track_effects
            })
            
            # Generate pattern based on element type and intent
            pattern_notes = self._generate_pattern_notes(
                element, 
                intent.duration_bars * 48,  # 48 ticks per bar
                intent.complexity,
                intent.energy_level
            )
            
            patterns.append({
                "track": element.capitalize(),
                "name": f"{element.capitalize()} Pattern",
                "notes": pattern_notes,
                "style": self._get_pattern_style(element, intent)
            })
        
        # Add master effects
        if intent.effects_intensity > 0.5:
            effects.append({
                "type": "compressor",
                "target": "master",
                "params": {
                    "threshold": -12,
                    "ratio": 4,
                    "attack": 5,
                    "release": 100
                }
            })
        
        mix_settings = {
            "tempo": tempo,
            "master_volume": 100
        }
        
        arrangement = {
            "loop_length": intent.duration_bars * 48,
            "sections": [{"name": "main", "start": 0, "length": intent.duration_bars * 48}]
        }
        
        return ProductionPlan(
            tracks=tracks,
            patterns=patterns,
            effects=effects,
            automation=[],
            mix_settings=mix_settings,
            arrangement=arrangement
        )
    
    def _generate_pattern_notes(self, element: str, length: int, 
                               complexity: float, energy: float) -> List[Dict]:
        """Generate pattern notes based on element type and parameters"""
        notes = []
        
        if element == "kick":
            # Generate kick pattern
            if complexity < 0.4:  # Simple four-on-the-floor
                positions = [i * 48 for i in range(length // 48)]
            else:  # More complex pattern
                positions = []
                for bar in range(length // 48):
                    bar_start = bar * 48
                    positions.append(bar_start)  # Downbeat
                    if complexity > 0.5:
                        positions.append(bar_start + 24)  # And of 2
                    if complexity > 0.7:
                        positions.append(bar_start + 36)  # And of 3.5
                    if energy > 0.7 and complexity > 0.6:
                        positions.append(bar_start + 42)  # Syncopation
            
            for pos in positions:
                notes.append({
                    "pitch": 36,  # C2
                    "position": pos,
                    "length": 12,
                    "velocity": int(80 + energy * 40)
                })
        
        elif element == "bass":
            # Generate bassline
            root_note = 36  # C2
            
            if complexity < 0.4:  # Simple pattern
                for i in range(0, length, 48):
                    notes.append({
                        "pitch": root_note,
                        "position": i,
                        "length": 36,
                        "velocity": int(60 + energy * 30)
                    })
            else:  # Rolling bassline
                for i in range(0, length, 12):
                    # Add some variation
                    if i % 48 == 0:
                        pitch = root_note
                    elif i % 48 == 24 and complexity > 0.6:
                        pitch = root_note + 7  # Fifth
                    else:
                        pitch = root_note
                    
                    notes.append({
                        "pitch": pitch,
                        "position": i,
                        "length": 10,
                        "velocity": int(50 + energy * 40 + random.randint(-10, 10))
                    })
        
        elif element in ["hats", "hat", "hihat"]:
            # Generate hi-hat pattern
            if complexity < 0.4:  # Minimal
                positions = [24, 72, 120, 168] if length >= 192 else [24, 72]
            else:  # More complex
                positions = []
                for i in range(0, length, 12):
                    if i % 48 in [12, 24, 36] or (complexity > 0.7 and i % 48 in [6, 18, 30, 42]):
                        positions.append(i)
            
            for pos in positions:
                notes.append({
                    "pitch": 42,  # F#2 (closed hat)
                    "position": pos,
                    "length": 6,
                    "velocity": int(40 + energy * 30 + random.randint(-5, 10))
                })
        
        elif element == "snare":
            # Generate snare pattern
            for bar in range(length // 48):
                bar_start = bar * 48
                notes.append({
                    "pitch": 38,  # D2
                    "position": bar_start + 24,  # On beat 2
                    "length": 12,
                    "velocity": int(70 + energy * 30)
                })
                if complexity > 0.5:
                    notes.append({
                        "pitch": 38,
                        "position": bar_start + 36,  # Syncopation
                        "length": 6,
                        "velocity": int(50 + energy * 20)
                    })
        
        return notes
    
    def _get_pattern_style(self, element: str, intent: MusicalIntent) -> str:
        """Describe the pattern style based on element and intent"""
        if element == "kick":
            if intent.complexity < 0.4:
                return "four-on-the-floor"
            elif intent.energy_level > 0.7:
                return "driving"
            else:
                return "syncopated"
        elif element == "bass":
            if intent.complexity > 0.6:
                return "rolling"
            elif "acid" in intent.characteristics:
                return "acid"
            else:
                return "simple"
        elif element in ["hats", "hat", "hihat"]:
            if intent.complexity < 0.4:
                return "minimal"
            else:
                return "shuffled"
        else:
            return "standard"
    
    def execute_production_plan(self, plan: ProductionPlan) -> str:
        """Execute the production plan using LMMS controller"""
        
        # Create new project
        self.controller.create_new_project()
        
        # Set tempo
        if plan.mix_settings.get("tempo"):
            self.controller.set_tempo(plan.mix_settings["tempo"])
        
        # Create tracks
        for track_def in plan.tracks:
            track_name = track_def["name"]
            
            # Add track
            self.controller.add_track(track_name, TrackType.INSTRUMENT)
            
            # Set instrument
            self.controller.set_instrument(track_name, track_def["instrument"])
            
            # Set instrument parameters
            for param, value in track_def["parameters"].items():
                self.controller.set_instrument_parameter(track_name, param, value)
            
            # Set volume and panning
            if "volume" in track_def:
                self.controller.set_volume(track_name, track_def["volume"])
            if "panning" in track_def:
                self.controller.set_panning(track_name, track_def["panning"])
            
            # Add effects
            for effect in track_def.get("effects", []):
                self.controller.add_effect(track_name, effect["type"], **effect.get("params", {}))
        
        # Create patterns
        for pattern_def in plan.patterns:
            track_name = pattern_def["track"]
            pattern_name = pattern_def["name"]
            
            # Add pattern
            pattern = self.controller.add_pattern(track_name, pattern_name, 0, 
                                                 plan.arrangement["loop_length"])
            
            # Add notes
            if pattern:
                for note_data in pattern_def["notes"]:
                    note = Note(
                        pitch=note_data["pitch"],
                        position=note_data["position"],
                        length=note_data["length"],
                        velocity=note_data.get("velocity", 100),
                        panning=note_data.get("panning", 0)
                    )
                    pattern.append(note.to_xml())
        
        # Add master effects
        for effect in plan.effects:
            if effect.get("target") == "master":
                # Would add to master channel
                pass
        
        # Set loop points
        if plan.arrangement.get("loop_length"):
            self.controller.set_loop_points(0, plan.arrangement["loop_length"])
        
        # Generate unique filename
        import time
        timestamp = int(time.time())
        filename = f"ai_generated_{timestamp}.mmp"
        
        # Save project
        self.controller.save_project(filename)
        
        return filename
    
    def create_music(self, request: str) -> str:
        """
        Main entry point: interpret request and create music
        Returns the path to the generated project file
        """
        print(f"Interpreting request: {request}")
        
        # Interpret the musical intent
        intent = self.interpret_request(request)
        print(f"Interpreted intent: Genre={intent.genre}, Mood={intent.mood}, "
              f"Energy={intent.energy_level:.2f}, Effects={intent.effects_intensity:.2f}")
        
        # Generate production plan
        print("Generating production plan...")
        plan = self.generate_production_plan(intent)
        print(f"Plan: {len(plan.tracks)} tracks, {len(plan.patterns)} patterns")
        
        # Execute the plan
        print("Executing production plan...")
        project_file = self.execute_production_plan(plan)
        
        print(f"Music created successfully: {project_file}")
        return project_file


def main():
    """Command-line interface for testing"""
    import sys
    
    if len(sys.argv) < 2:
        print("""
LMMS AI Brain - Intelligent Music Production System

Usage:
  python lmms_ai_brain.py "<musical request>"
  
Examples:
  python lmms_ai_brain.py "create a dark techno beat with heavy distortion"
  python lmms_ai_brain.py "make minimal house music with a groovy bassline"
  python lmms_ai_brain.py "generate an aggressive dnb track with complex drums"
  python lmms_ai_brain.py "create ambient pad sounds with lots of reverb"
  
Set OPENAI_API_KEY environment variable for GPT-powered intelligence.
Without API key, falls back to rule-based system.
        """)
        sys.exit(1)
    
    request = " ".join(sys.argv[1:])
    
    # Create AI brain
    brain = LMMSAIBrain()
    
    # Create music
    try:
        project_file = brain.create_music(request)
        print(f"\n✓ Success! Open '{project_file}' in LMMS to hear your music.")
    except Exception as e:
        print(f"\n✗ Error creating music: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()